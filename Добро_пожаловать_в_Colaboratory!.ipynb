{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowSokol/test_Sokolov/blob/main/%D0%94%D0%BE%D0%B1%D1%80%D0%BE_%D0%BF%D0%BE%D0%B6%D0%B0%D0%BB%D0%BE%D0%B2%D0%B0%D1%82%D1%8C_%D0%B2_Colaboratory!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip --version"
      ],
      "metadata": {
        "id": "3XLiKKQD6vJf",
        "outputId": "328a15da-554e-429b-9ce8-16d407adfffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка PySpark"
      ],
      "metadata": {
        "id": "nwrTTO7K829Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zwFnJsE6vjf8",
        "outputId": "a0aab834-b076-4137-cbe1-a0d6619cd27f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark стоит"
      ],
      "metadata": {
        "id": "puivqRE186hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "\n",
        "\n",
        "# Настройка Spark\n",
        "\n",
        "conf = SparkConf().setAppName(\"Simple RDD Example\").setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Создание RDD из списка чисел\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Трансформации: Фильтрация чётных чисел\n",
        "\n",
        "even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Действие: Подсчёт суммы чётных чисел\n",
        "\n",
        "sum_even_numbers = even_numbers_rdd.sum()\n",
        "\n",
        "\n",
        "\n",
        "# Вывод результата\n",
        "\n",
        "print(\"Чётные числа:\", even_numbers_rdd.collect())\n",
        "\n",
        "print(\"Сумма чётных чисел:\", sum_even_numbers)\n",
        "\n",
        "\n",
        "\n",
        "a = 5 + 6\n",
        "\n",
        "print(a)\n",
        "\n",
        "# Остановка SparkContext\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "ylD2JJ6w8y--",
        "outputId": "e82ffe6e-0508-4547-fc4c-b140976407bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Read CSV Example, master=local[*]) created by getOrCreate at <ipython-input-1-fea8a025f6cf>:4 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-316a7e678000>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Simple RDD Example\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Read CSV Example, master=local[*]) created by getOrCreate at <ipython-input-1-fea8a025f6cf>:4 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подтверждаем что PySpark стоит"
      ],
      "metadata": {
        "id": "qXHfrjzv8-Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "ypXQ9lwaXIEe",
        "outputId": "2d435a54-d49e-474c-e96a-bf3eae3d87bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-35.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-35.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-35.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "num_records = 100000\n",
        "\n",
        "http_methods = ['GET', 'POST', 'PUT', 'DELETE']\n",
        "response_codes = [200, 301, 404, 500]\n",
        "\n",
        "file_path = \"web_server_logs.csv\"\n",
        "\n",
        "with open(file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ip', 'timestamp', 'method', 'url', 'response_code', 'response_size'])\n",
        "\n",
        "    for _ in range(num_records):\n",
        "        ip = fake.ipv4()\n",
        "        timestamp = fake.date_time_this_year().isoformat()\n",
        "        method = random.choice(http_methods)\n",
        "        url = fake.uri_path()\n",
        "        response_code = random.choice(response_codes)\n",
        "        response_size = random.randint(100, 10000)\n",
        "\n",
        "        writer.writerow([ip, timestamp, method, url, response_code, response_size])\n",
        "\n",
        "print(f\"Сгенерировано {num_records} записей и сохранено в {file_path}\")"
      ],
      "metadata": {
        "id": "0F6k6XMtXchf",
        "outputId": "18b124a2-3e9e-4a82-e93e-3827ced3a7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сгенерировано 100000 записей и сохранено в web_server_logs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерировали записи и получили файл\n"
      ],
      "metadata": {
        "id": "fbl7TqGjXkeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ИТОГОВОЕ ЗАДАНИЕ по PySpark**"
      ],
      "metadata": {
        "id": "V1yzt8Q4Bbiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание SparkSession\n",
        "spark = SparkSession.builder.appName(\"Read CSV Example\").getOrCreate()\n",
        "\n",
        "# Чтение CSV-файла\n",
        "df = spark.read.csv(\"/content/web_server_logs.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Печать схемы DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "\n",
        "print(\"Top 10 active IP adress:\")\n",
        "top_ip = spark.sql(\"SELECT ip, COUNT(ip) AS request_count FROM df GROUP BY ip\").limit(10).show()\n",
        "\n",
        "print(\"Request count by HTTP method:\")\n",
        "method = spark.sql(\"SELECT method, COUNT(method) AS method_count FROM df GROUP BY method\").show()\n",
        "\n",
        "count_code = spark.sql(\"SELECT COUNT(response_code) AS count_response_code FROM df WHERE response_code = 404\").show()\n",
        "#print(\"number of 404 response codes:\", count_code)\n",
        "\n",
        "print(\"total response size by day:\")\n",
        "response_size = spark.sql(\"SELECT date, SUM(response_size) as total_response_size FROM (SELECT to_date(timestamp, 'yyyy-MM-dd') as date, response_size FROM df) AS T GROUP BY date ORDER BY date\").show()"
      ],
      "metadata": {
        "id": "g3elmjitXq9x",
        "outputId": "5fa80f4a-cde9-4400-ffad-ab7600c8aa89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ip: string (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- method: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            " |-- response_code: integer (nullable = true)\n",
            " |-- response_size: integer (nullable = true)\n",
            "\n",
            "Top 10 active IP adress:\n",
            "+--------------+-------------+\n",
            "|            ip|request_count|\n",
            "+--------------+-------------+\n",
            "|  39.43.102.57|            1|\n",
            "| 114.44.72.220|            1|\n",
            "|200.166.61.168|            1|\n",
            "| 46.95.234.188|            1|\n",
            "|95.173.211.205|            1|\n",
            "|158.113.136.77|            1|\n",
            "| 58.163.71.113|            1|\n",
            "|  4.109.129.72|            1|\n",
            "| 218.120.8.111|            1|\n",
            "|  8.187.55.222|            1|\n",
            "+--------------+-------------+\n",
            "\n",
            "Request count by HTTP method:\n",
            "+------+------------+\n",
            "|method|method_count|\n",
            "+------+------------+\n",
            "|  POST|       25257|\n",
            "|DELETE|       25005|\n",
            "|   PUT|       24765|\n",
            "|   GET|       24973|\n",
            "+------+------------+\n",
            "\n",
            "+-------------------+\n",
            "|count_response_code|\n",
            "+-------------------+\n",
            "|              25000|\n",
            "+-------------------+\n",
            "\n",
            "total response size by day:\n",
            "+----------+-------------------+\n",
            "|      date|total_response_size|\n",
            "+----------+-------------------+\n",
            "|2025-01-01|           20968929|\n",
            "|2025-01-02|           21473157|\n",
            "|2025-01-03|           22009603|\n",
            "|2025-01-04|           21030826|\n",
            "|2025-01-05|           21415994|\n",
            "|2025-01-06|           21364001|\n",
            "|2025-01-07|           20946712|\n",
            "|2025-01-08|           21123079|\n",
            "|2025-01-09|           21961027|\n",
            "|2025-01-10|           22125538|\n",
            "|2025-01-11|           21467434|\n",
            "|2025-01-12|           21582003|\n",
            "|2025-01-13|           22469386|\n",
            "|2025-01-14|           22227941|\n",
            "|2025-01-15|           21340347|\n",
            "|2025-01-16|           20971407|\n",
            "|2025-01-17|           21446435|\n",
            "|2025-01-18|           21314129|\n",
            "|2025-01-19|           21782903|\n",
            "|2025-01-20|           21368171|\n",
            "+----------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Добро пожаловать в Colaboratory!",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}